{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18aeb6ea",
   "metadata": {},
   "source": [
    "# Python vs. C++ - Why we choose Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fb082",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524aea6",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PyYAML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbe129",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e515b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f2e3a",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28214030",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f56e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bd1b7",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986924fc",
   "metadata": {},
   "source": [
    "## > Dataset Acquisition & Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dad934",
   "metadata": {},
   "source": [
    "Our idea here is try to understand the distribution of the data, how the classes are distributed and if all resolution are the same. This step is important because will direct us about how to lead with the data on the nexts steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd679566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "# Path to the folder containing the extracted images\n",
    "root_dir = \"MIR_DATASETS_B\"\n",
    "output_csv = \"inventory.csv\"\n",
    "\n",
    "rows = []\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for f in filenames:\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            path = os.path.join(dirpath, f)\n",
    "            label = os.path.basename(dirpath)  # Folder name is used as the label\n",
    "            img = cv2.imread(path)\n",
    "            h, w = img.shape[:2]  # Get image height and width\n",
    "            size_kb = round(os.path.getsize(path) / 1024, 2)  # File size in kilobytes\n",
    "            rows.append([path, label, w, h, size_kb])\n",
    "\n",
    "# Save image metadata to a CSV file\n",
    "with open(output_csv, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"class\", \"width\", \"height\", \"size_kb\"])\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"inventory.csv\")\n",
    "\n",
    "# Number of images by class\n",
    "class_counts = df['class'].value_counts().reset_index()\n",
    "class_counts.columns = [\"class\", \"n_imagens\"]\n",
    "\n",
    "# Average resolution Â± Ïƒ\n",
    "df[\"resoluÃ§Ã£o\"] = df[\"width\"].astype(int) * df[\"height\"].astype(int)\n",
    "mean_res = df.groupby(\"class\")[\"resoluÃ§Ã£o\"].mean()\n",
    "std_res = df.groupby(\"class\")[\"resoluÃ§Ã£o\"].std()\n",
    "\n",
    "# Example of classes\n",
    "example_paths = df.groupby(\"class\")[\"path\"].apply(lambda x: x.sample(3, random_state=42).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6661c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single summary table\n",
    "summary_table = class_counts.set_index(\"class\")\n",
    "summary_table[\"mean_resolution\"] = mean_res\n",
    "summary_table[\"std_resolution\"] = std_res\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064b007",
   "metadata": {},
   "source": [
    "## > Pre image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ee550",
   "metadata": {},
   "source": [
    "It's important to normalize, redimension and convert all images to RGB to have something more standardized. Without this step, we will have problems applying the algorithms in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d518d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "target_size = config[\"preprocessing\"][\"target_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "target_size = config[\"preprocessing\"][\"target_size\"]\n",
    "\n",
    "# def preprocess(img):\n",
    "#     \"\"\"\n",
    "#     Preprocesses an image:\n",
    "#     - Converts BGR to RGB\n",
    "#     - Resizes to target size with aspect ratio preservation and padding\n",
    "#     - Normalizes pixel values to float32 range [0, 1]\n",
    "#     \"\"\"\n",
    "#     # Convert to RGB\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Get original dimensions\n",
    "#     h, w = img_rgb.shape[:2]\n",
    "#     scale = min(target_size / h, target_size / w)\n",
    "#     new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "#     # Resize with aspect ratio\n",
    "#     resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#     # Add padding to reach target size\n",
    "#     delta_w = target_size - new_w\n",
    "#     delta_h = target_size - new_h\n",
    "#     top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "#     left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "#     padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "#     # Normalize to [0, 1]\n",
    "#     normalized = padded.astype(np.float32) / 255.0\n",
    "\n",
    "#     return normalized\n",
    "\n",
    "def preprocess(img, target_size=256):\n",
    "    \"\"\"Resize + pad + normalize image to match interface.\"\"\"\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    scale = min(target_size / h, target_size / w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    delta_w = target_size - new_w\n",
    "    delta_h = target_size - new_h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    normalized = padded.astype(np.float32) / 255.0\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess_shape_and_range():\n",
    "    # Create dummy image with random size and color\n",
    "    dummy_img = np.random.randint(0, 256, (300, 150, 3), dtype=np.uint8)\n",
    "\n",
    "    processed = preprocess(dummy_img)\n",
    "\n",
    "    # Check shape\n",
    "    assert processed.shape == (256, 256, 3)\n",
    "\n",
    "    # Check range\n",
    "    assert processed.dtype == np.float32\n",
    "    assert processed.min() >= 0.0\n",
    "    assert processed.max() <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6ec7e",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa11f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image in BGR format\n",
    "original_img = cv2.imread(\"MIR_DATASETS_B/MIR_DATASETS_B/araignees/barn spider/0_0_araignees_barnspider_1.jpg\")\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_img = preprocess(original_img)\n",
    "\n",
    "# Convert original BGR to RGB for display\n",
    "original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display both images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(original_rgb)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(processed_img)\n",
    "axes[1].set_title(\"Preprocessed (256x256)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa863da",
   "metadata": {},
   "source": [
    "## > Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc90ffd",
   "metadata": {},
   "source": [
    "### Extract descriptors functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    hist = cv2.calcHist([img_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256]*3)\n",
    "    hist = cv2.normalize(hist, hist, norm_type=cv2.NORM_L1)  # normalizaÃ§Ã£o correta\n",
    "    return hist.flatten()\n",
    "\n",
    "\n",
    "def extract_orb(img):\n",
    "    \"\"\"\n",
    "    Extract ORB descriptors from a raw or normalized image.\n",
    "    Ensures correct input type and shape.\n",
    "    \"\"\"\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    orb = cv2.ORB_create(nfeatures=500)\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "\n",
    "    return descriptors  # can be None\n",
    "\n",
    "\n",
    "def extract_sift(img):\n",
    "    \"\"\"\n",
    "    Extract SIFT descriptors from a raw or normalized image.\n",
    "    Ensures correct input type and shape.\n",
    "    \"\"\"\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    return descriptors  # can be None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21b83b",
   "metadata": {},
   "source": [
    "## > Extraction and Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e2936",
   "metadata": {},
   "source": [
    "### Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d83926a",
   "metadata": {},
   "source": [
    "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "           â”‚     dataset_dir (root)     â”‚\n",
    "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Coletar caminhos  â”‚\n",
    "              â”‚ de todas imagens  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  Paralelizar com  â”‚\n",
    "              â”‚  multiprocessing  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Extrair: color_hist, ORB, SIFT â”‚\n",
    "        â”‚ Salvar .npy (por imagem)       â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Atualizar `features_meta.parquet` â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add375a",
   "metadata": {},
   "source": [
    "### Feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4a5d77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# Worker function that runs in main process or subprocess\n",
    "# def extract_features_worker(args):\n",
    "#     import gc\n",
    "#     img_path, output_dir = args\n",
    "#     print(f\"[INFO] Processando: {img_path}\")\n",
    "\n",
    "#     try:\n",
    "#         img = cv2.imread(img_path)\n",
    "#         if img is None or img.size == 0:\n",
    "#             print(f\"[ERRO] Imagem invÃ¡lida (cv2.imread retornou None): {img_path}\")\n",
    "#             return None\n",
    "\n",
    "#         orb = cv2.ORB_create()\n",
    "#         sift = cv2.SIFT_create()\n",
    "\n",
    "#         try:\n",
    "#             hist = cv2.calcHist([img], [0, 1, 2], None, [8]*3, [0, 256]*3).flatten()\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERRO] Falha ao calcular histograma: {img_path}: {e}\")\n",
    "#             hist = np.zeros(512, dtype=np.float32)\n",
    "\n",
    "#         try:\n",
    "#             _, orb_desc = orb.detectAndCompute(img, None)\n",
    "#             if orb_desc is None:\n",
    "#                 orb_desc = np.zeros((0, 32), dtype=np.uint8)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERRO] ORB em {img_path}: {e}\")\n",
    "#             orb_desc = np.zeros((0, 32), dtype=np.uint8)\n",
    "\n",
    "#         try:\n",
    "#             _, sift_desc = sift.detectAndCompute(img, None)\n",
    "#             if sift_desc is None:\n",
    "#                 sift_desc = np.zeros((0, 128), dtype=np.float32)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERRO] SIFT em {img_path}: {e}\")\n",
    "#             sift_desc = np.zeros((0, 128), dtype=np.float32)\n",
    "\n",
    "#         base = Path(img_path).stem\n",
    "#         np.save(f\"{output_dir}/{base}_hist.npy\", hist)\n",
    "#         np.save(f\"{output_dir}/{base}_orb.npy\", orb_desc)\n",
    "#         np.save(f\"{output_dir}/{base}_sift.npy\", sift_desc)\n",
    "\n",
    "#         del img, hist, orb_desc, sift_desc\n",
    "#         gc.collect()\n",
    "\n",
    "#         return {\n",
    "#             \"file\": img_path,\n",
    "#             \"hist_file\": f\"{base}_hist.npy\",\n",
    "#             \"orb_file\": f\"{base}_orb.npy\",\n",
    "#             \"sift_file\": f\"{base}_sift.npy\"\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[FATAL] Erro inesperado em {img_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "def extract_features_worker(args):\n",
    "    import gc\n",
    "    img_path, output_dir = args\n",
    "    print(f\"[INFO] Processing: {img_path}\")\n",
    "\n",
    "    try:\n",
    "        img_bgr = cv2.imread(img_path)\n",
    "        if img_bgr is None or img_bgr.size == 0:\n",
    "            print(f\"[ERROR] Invalid image: {img_path}\")\n",
    "            return None\n",
    "\n",
    "        # Apply same preprocessing as interface\n",
    "        img_preprocessed = preprocess(img_bgr, target_size=256)\n",
    "        img_for_descriptors = (img_preprocessed * 255).astype(np.uint8)  # Convert back to 0-255 uint8\n",
    "\n",
    "        # Extract histogram (from RGB normalized)\n",
    "        img_rgb = cv2.cvtColor(img_for_descriptors, cv2.COLOR_BGR2RGB)\n",
    "        hist = cv2.calcHist([img_rgb], [0, 1, 2], None, [8, 8, 8], [0, 256]*3)\n",
    "        hist = cv2.normalize(hist, hist, norm_type=cv2.NORM_L1).flatten()\n",
    "\n",
    "        # ORB\n",
    "        orb = cv2.ORB_create(nfeatures=500)\n",
    "        gray_orb = cv2.cvtColor(img_for_descriptors, cv2.COLOR_RGB2GRAY)\n",
    "        _, orb_desc = orb.detectAndCompute(gray_orb, None)\n",
    "        if orb_desc is None:\n",
    "            orb_desc = np.zeros((0, 32), dtype=np.uint8)\n",
    "\n",
    "        # SIFT\n",
    "        sift = cv2.SIFT_create()\n",
    "        gray_sift = cv2.cvtColor(img_for_descriptors, cv2.COLOR_RGB2GRAY)\n",
    "        _, sift_desc = sift.detectAndCompute(gray_sift, None)\n",
    "        if sift_desc is None:\n",
    "            sift_desc = np.zeros((0, 128), dtype=np.float32)\n",
    "\n",
    "        base = Path(img_path).stem\n",
    "        np.save(f\"{output_dir}/{base}_hist.npy\", hist)\n",
    "        np.save(f\"{output_dir}/{base}_orb.npy\", orb_desc)\n",
    "        np.save(f\"{output_dir}/{base}_sift.npy\", sift_desc)\n",
    "\n",
    "        del img_bgr, img_preprocessed, orb_desc, sift_desc, hist\n",
    "        gc.collect()\n",
    "\n",
    "        return {\n",
    "            \"file\": img_path,\n",
    "            \"hist_file\": f\"{base}_hist.npy\",\n",
    "            \"orb_file\": f\"{base}_orb.npy\",\n",
    "            \"sift_file\": f\"{base}_sift.npy\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Unexpected error with {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Manager class\n",
    "# class FeatureExtractorManager:\n",
    "#     def run_on_batch(self, image_paths, output_dir):\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         args = [(img_path, output_dir) for img_path in image_paths]\n",
    "#         results = []\n",
    "\n",
    "#         for arg in tqdm(args, desc=\"Processando lote\"):\n",
    "#             result = extract_features_worker(arg)\n",
    "#             if result is not None:\n",
    "#                 results.append(result)\n",
    "#             gc.collect()  # liberaÃ§Ã£o manual de memÃ³ria\n",
    "\n",
    "#         df = pd.DataFrame(results)\n",
    "#         return df\n",
    "#     def run(self, dataset_dir, output_dir, n_jobs=None):\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         all_imgs = [str(p) for p in Path(dataset_dir).rglob(\"*.jpg\")]\n",
    "#         n_jobs = n_jobs or cpu_count()\n",
    "\n",
    "#         args = [(img_path, output_dir) for img_path in all_imgs]\n",
    "\n",
    "#         results = []\n",
    "#         if n_jobs == 1:\n",
    "#             # Sequential loop (safe for notebooks)\n",
    "#             for arg in tqdm(args, desc=\"Processing images (no parallel)\"):\n",
    "#                 result = extract_features_worker(arg)\n",
    "#                 if result is not None:\n",
    "#                     results.append(result)\n",
    "#         else:\n",
    "#             # Parallel execution\n",
    "#             with Pool(n_jobs) as pool:\n",
    "#                 results = list(tqdm(pool.imap(extract_features_worker, args), total=len(all_imgs)))\n",
    "#                 results = [r for r in results if r is not None]\n",
    "\n",
    "#         # Save metadata\n",
    "#         df = pd.DataFrame(results)\n",
    "#         df.to_parquet(os.path.join(output_dir, \"features_meta.parquet\"))\n",
    "class FeatureExtractorManager:\n",
    "    def run_on_batch(self, image_paths, output_dir):\n",
    "        \"\"\"Sequential feature extraction for a list of image paths.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        args = [(img_path, output_dir) for img_path in image_paths]\n",
    "        results = []\n",
    "\n",
    "        for arg in tqdm(args, desc=\"ðŸ”„ Processing images (sequential)\"):\n",
    "            result = extract_features_worker(arg)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "            gc.collect()\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        return df\n",
    "\n",
    "    def run(self, dataset_dir, output_dir, n_jobs=None):\n",
    "        \"\"\"Full directory feature extraction with optional multiprocessing.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        all_imgs = [str(p) for p in Path(dataset_dir).rglob(\"*.jpg\")]\n",
    "        n_jobs = n_jobs or cpu_count()\n",
    "\n",
    "        args = [(img_path, output_dir) for img_path in all_imgs]\n",
    "        results = []\n",
    "\n",
    "        if n_jobs == 1:\n",
    "            for arg in tqdm(args, desc=\"ðŸ”„ Processing images (no parallel)\"):\n",
    "                result = extract_features_worker(arg)\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "        else:\n",
    "            with Pool(n_jobs) as pool:\n",
    "                for result in tqdm(pool.imap(extract_features_worker, args), total=len(args)):\n",
    "                    if result is not None:\n",
    "                        results.append(result)\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_parquet(os.path.join(output_dir, \"features_meta.parquet\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b6a634",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_path = \"MIR_DATASETS_B/MIR_DATASETS_B/araignees/barn spider/0_0_araignees_barnspider_1.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- Color Histogram ---\n",
    "hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256]*3).flatten()\n",
    "colors = ('r', 'g', 'b')\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, color in enumerate(colors):\n",
    "    hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist, color=color)\n",
    "plt.title(\"Color Histogram (RGB channels)\")\n",
    "plt.xlabel(\"Pixel value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- ORB Keypoints ---\n",
    "orb = cv2.ORB_create()\n",
    "kp_orb, desc_orb = orb.detectAndCompute(img, None)\n",
    "img_kp_orb = cv2.drawKeypoints(img, kp_orb, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv2.cvtColor(img_kp_orb, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"ORB Keypoints\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# --- SIFT Keypoints ---\n",
    "sift = cv2.SIFT_create()\n",
    "kp_sift, desc_sift = sift.detectAndCompute(img, None)\n",
    "img_kp_sift = cv2.drawKeypoints(img, kp_sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv2.cvtColor(img_kp_sift, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"SIFT Keypoints\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Visualization & Output ---\n",
    "print(\"Color Histogram Shape:\", hist.shape)\n",
    "print(\"ORB Descriptor Shape:\", None if desc_orb is None else desc_orb.shape)\n",
    "print(\"SIFT Descriptor Shape:\", None if desc_sift is None else desc_sift.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf5cc1",
   "metadata": {},
   "source": [
    "## > Construction of the index with FAISS (IVF-PQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f95df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e68c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Root path where class folders are located\n",
    "root = Path(\"MIR_DATASETS_B/MIR_DATASETS_B\")\n",
    "\n",
    "# Initialize the feature extraction manager (assumed to be implemented elsewhere)\n",
    "fem = FeatureExtractorManager()\n",
    "\n",
    "# Iterate through each subdirectory (representing a class)\n",
    "for class_dir in root.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    input_dir = str(class_dir)\n",
    "    output_dir = str(Path(\"features_output\") / class_dir.name)\n",
    "\n",
    "    print(f\"\\nðŸŸ¢ Starting processing for class: {class_dir.name}\")\n",
    "    fem.run(input_dir, output_dir, n_jobs=1)\n",
    "    print(f\"âœ… Completed: {class_dir.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3ff1a",
   "metadata": {},
   "source": [
    "### Creating the matrix N x D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59793aa4",
   "metadata": {},
   "source": [
    "Getting the parameters and applying in Faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"features_output\")\n",
    "print(\"Subfolders:\", [p.name for p in root.iterdir() if p.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f166bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for folder in os.listdir(\"features_output\"):\n",
    "    path = os.path.join(\"features_output\", folder)\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"Checking: {folder}\")\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\"_hist.npy\") or file.endswith(\"_sift.npy\") or file.endswith(\"_orb.npy\"):\n",
    "                print(\"  Found descriptor file:\", file)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f20580",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.load(\"features_output/araignees/0_0_araignees_barnspider_0_hist.npy\")\n",
    "print(vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefd3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing class subfolders with features\n",
    "features_root = \"features_output\"\n",
    "index_output = \"index_store\"\n",
    "os.makedirs(index_output, exist_ok=True)\n",
    "\n",
    "# Supported descriptor types\n",
    "descriptor_types = [\"hist\", \"orb\", \"sift\"]\n",
    "\n",
    "# Target fixed lengths for each descriptor (after flattening)\n",
    "target_lengths = {\n",
    "    \"hist\": 512,           # 8x8x8 bins\n",
    "    \"orb\": 500 * 32,       # 500 keypoints Ã— 32 dims\n",
    "    \"sift\": 500 * 128      # 500 keypoints Ã— 128 dims\n",
    "}\n",
    "\n",
    "# Fix vector to a specific length by truncating or padding with zeros\n",
    "def fix_vector(vec, target_len):\n",
    "    flat = vec.flatten()\n",
    "    if flat.shape[0] >= target_len:\n",
    "        return flat[:target_len]\n",
    "    else:\n",
    "        padded = np.zeros(target_len, dtype=np.float32)\n",
    "        padded[:flat.shape[0]] = flat\n",
    "        return padded\n",
    "\n",
    "# Dictionary to collect descriptor vectors and IDs\n",
    "vectors = {desc: [] for desc in descriptor_types}\n",
    "ids = {desc: [] for desc in descriptor_types}\n",
    "\n",
    "# Step 1: Traverse all subfolders and collect .npy files by descriptor type\n",
    "for class_dir in Path(features_root).glob(\"*\"):\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "    print(f\">>> Entering class folder: {class_dir.name}\")\n",
    "\n",
    "    meta_path = class_dir / \"features_meta.parquet\"\n",
    "    if not meta_path.exists():\n",
    "        print(f\"[!] Missing metadata in: {class_dir}\")\n",
    "        continue\n",
    "\n",
    "    meta = pd.read_parquet(meta_path)\n",
    "\n",
    "    for npy_file in class_dir.glob(\"*.npy\"):\n",
    "        fname = npy_file.name\n",
    "        print(f\"  - Found: {npy_file.name}\")\n",
    "        for desc in descriptor_types:\n",
    "            if fname.endswith(f\"_{desc}.npy\"):\n",
    "                print(f\"    -> Matched descriptor: {desc}\")\n",
    "                try:\n",
    "                    vec = np.load(npy_file)\n",
    "                    if vec is None or len(vec.shape) == 0:\n",
    "                        print(\"    [!] Empty vector, skipping\")\n",
    "                        continue\n",
    "\n",
    "                    # Apply fix only for descriptors that need it\n",
    "                    vec = fix_vector(vec, target_lengths[desc])\n",
    "                    vectors[desc].append(vec.astype(np.float32))\n",
    "\n",
    "                    ids[desc].append({\n",
    "                        \"path\": str(npy_file),\n",
    "                        \"classe\": class_dir.name,\n",
    "                        \"vector_dim\": vec.shape[0]\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    [X] Error loading {npy_file}: {e}\")\n",
    "                break\n",
    "\n",
    "# Step 2: For each descriptor type, build FAISS index and save IDs\n",
    "for desc in descriptor_types:\n",
    "    vec_list = vectors[desc]\n",
    "    if not vec_list:\n",
    "        print(f\"[!] No vectors found for descriptor: {desc}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"âœ… Building FAISS index for: {desc} with {len(vec_list)} vectors\")\n",
    "\n",
    "    X = np.vstack(vec_list).astype(np.float32)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # Create FlatL2 index and add vectors\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(X)\n",
    "\n",
    "    # Save FAISS index\n",
    "    index_path = f\"{index_output}/index_{desc}.faiss\"\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # Save corresponding metadata\n",
    "    df_ids = pd.DataFrame(ids[desc])\n",
    "    ids_path = f\"{index_output}/ids_{desc}.parquet\"\n",
    "    df_ids.to_parquet(ids_path, index=False)\n",
    "\n",
    "    print(f\"ðŸ“ Saved {index_path} and {ids_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e8520",
   "metadata": {},
   "source": [
    "## > Similiarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab830e",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728be225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance(a: np.ndarray, b: np.ndarray, metric=\"cosine\") -> float:\n",
    "    \"\"\"\n",
    "    Computes the distance or similarity between two vectors using the specified metric.\n",
    "\n",
    "    Supported metrics:\n",
    "    - \"l2\"\n",
    "    - \"cosine\"\n",
    "    - \"chi2\"\n",
    "    - \"bhattacharyya\"\n",
    "\n",
    "    Parameters:\n",
    "        a (np.ndarray): First vector.\n",
    "        b (np.ndarray): Second vector.\n",
    "        metric (str): Metric name.\n",
    "\n",
    "    Returns:\n",
    "        float: Distance value (lower = more similar, except cosine where higher = more similar).\n",
    "    \"\"\"\n",
    "    a = a.astype(np.float32)\n",
    "    b = b.astype(np.float32)\n",
    "\n",
    "    if metric == \"l2\":\n",
    "        return np.linalg.norm(a - b)\n",
    "\n",
    "    elif metric == \"cosine\":\n",
    "        num = np.dot(a, b)\n",
    "        denom = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "        return 1 - (num / denom) if denom != 0 else 1.0\n",
    "\n",
    "    elif metric == \"chi2\":\n",
    "        eps = 1e-10\n",
    "        return 0.5 * np.sum(((a - b) ** 2) / (a + b + eps))\n",
    "\n",
    "    elif metric == \"bhattacharyya\":\n",
    "        # Assumes histograms are normalized\n",
    "        return -np.log(np.sum(np.sqrt(a * b)) + 1e-10)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32011847",
   "metadata": {},
   "source": [
    "#### Comparing Faiss vs. NumPY (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS L2 index and add a vector\n",
    "d = 4\n",
    "index = faiss.IndexFlatL2(d)\n",
    "x = np.random.rand(1, d).astype(np.float32)\n",
    "index.add(x)\n",
    "\n",
    "# Query with itself\n",
    "D, I = index.search(x, 1)\n",
    "faiss_l2 = np.sqrt(D[0][0])  # FAISS returns squared L2\n",
    "\n",
    "# Compare with our implementation\n",
    "manual_l2 = distance(x[0], x[0], metric=\"l2\")\n",
    "\n",
    "print(f\"FAISS L2:   {faiss_l2}\")\n",
    "print(f\"Manual L2:  {manual_l2}\")\n",
    "print(f\"Diff:       {abs(faiss_l2 - manual_l2)}\")\n",
    "assert abs(faiss_l2 - manual_l2) < 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc558b",
   "metadata": {},
   "source": [
    "## > Query with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map descriptor type to its extractor and index settings\n",
    "DESCRIPTORS = {\n",
    "    \"hist\": {\n",
    "        \"extractor\": extract_color_histogram,\n",
    "        \"index_path\": \"index_store/index_hist.faiss\",\n",
    "        \"ids_path\": \"index_store/ids_hist.parquet\",\n",
    "        \"metric\": \"l2\"\n",
    "    },\n",
    "    \"orb\": {\n",
    "        \"extractor\": extract_orb,\n",
    "        \"index_path\": \"index_store/index_orb.faiss\",\n",
    "        \"ids_path\": \"index_store/ids_orb.parquet\",\n",
    "        \"metric\": \"l2\"\n",
    "    },\n",
    "    \"sift\": {\n",
    "        \"extractor\": extract_sift,\n",
    "        \"index_path\": \"index_store/index_sift.faiss\",\n",
    "        \"ids_path\": \"index_store/ids_sift.parquet\",\n",
    "        \"metric\": \"l2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fixed vector lengths used during FAISS index creation\n",
    "TARGET_LENGTHS = {\n",
    "    \"hist\": 512,\n",
    "    \"orb\": 500 * 32,\n",
    "    \"sift\": 500 * 128\n",
    "}\n",
    "\n",
    "def fix_vector(vec, target_len):\n",
    "    flat = vec.flatten()\n",
    "    if flat.shape[0] >= target_len:\n",
    "        return flat[:target_len]\n",
    "    else:\n",
    "        padded = np.zeros(target_len, dtype=np.float32)\n",
    "        padded[:flat.shape[0]] = flat\n",
    "        return padded\n",
    "\n",
    "def query_image(image_path, top_k=10, ensemble_config=None):\n",
    "    \"\"\"\n",
    "    Queries one or more FAISS indexes for the given image.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Path to the query image.\n",
    "        top_k (int): Number of results to return.\n",
    "        ensemble_config (dict): Weights for each descriptor. \n",
    "                                If None, uses only 'hist'.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with: image_id, score (distance), classe.\n",
    "    \"\"\"\n",
    "    if ensemble_config is None:\n",
    "        ensemble_config = {\"hist\": 1.0}\n",
    "\n",
    "    # Normalize path\n",
    "    image_path = str(Path(image_path).resolve())\n",
    "\n",
    "    # Load and preprocess image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"[X] Failed to load image: {image_path}\")\n",
    "    img = preprocess(img)\n",
    "\n",
    "    combined_scores = {}\n",
    "    combined_meta = {}\n",
    "\n",
    "    for desc, weight in ensemble_config.items():\n",
    "        if desc not in DESCRIPTORS:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Extract feature vector\n",
    "            vec = DESCRIPTORS[desc][\"extractor\"](img)\n",
    "\n",
    "            # Skip if extraction failed\n",
    "            if vec is None or vec.size == 0:\n",
    "                print(f\"[!] Descriptor '{desc}' returned None or empty for image {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            vec = fix_vector(vec, TARGET_LENGTHS[desc])\n",
    "            vec = vec.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "            # Load index and metadata\n",
    "            index = faiss.read_index(DESCRIPTORS[desc][\"index_path\"])\n",
    "            ids_df = pd.read_parquet(DESCRIPTORS[desc][\"ids_path\"])\n",
    "\n",
    "            # Search\n",
    "            distances, indices = index.search(vec, top_k * 5)\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx == -1:\n",
    "                    continue\n",
    "                item = ids_df.iloc[idx]\n",
    "                key = item[\"path\"]\n",
    "                cls = item[\"classe\"]\n",
    "                score = distances[0][i]\n",
    "\n",
    "                if key not in combined_scores:\n",
    "                    combined_scores[key] = 0\n",
    "                    combined_meta[key] = {\"classe\": cls}\n",
    "                combined_scores[key] += weight * score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[X] Error processing descriptor '{desc}' for image {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Aggregate and sort results\n",
    "    results = [\n",
    "        {\"image_id\": key, \"score\": combined_scores[key], \"classe\": combined_meta[key][\"classe\"]}\n",
    "        for key in combined_scores\n",
    "    ]\n",
    "    results = sorted(results, key=lambda x: x[\"score\"])[:top_k]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df65c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "results = query_image(\"MIR_DATASETS_B/MIR_DATASETS_B/chiens/Chihuahua/1_3_chiens_Chihuahua_1271.jpg\", top_k=5, ensemble_config={\"hist\": 0.5, \"orb\": 0.5})\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fef7c",
   "metadata": {},
   "source": [
    "#### Seeing the differences, and why is so wrong the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_query_results(query_path, results, title=\"Top-5 Similar Images\"):\n",
    "    \"\"\"\n",
    "    Displays the query image and the top-k results with scores and classes.\n",
    "\n",
    "    Parameters:\n",
    "        query_path (str): Path to the query image.\n",
    "        results (list): List of result dicts from query_image().\n",
    "    \"\"\"\n",
    "    n = len(results)\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Show query image\n",
    "    query_img = cv2.imread(query_path)\n",
    "    query_img = cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(1, n + 1, 1)\n",
    "    plt.imshow(query_img)\n",
    "    plt.title(\"Query\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Root folder of original images\n",
    "    original_root = Path(\"MIR_DATASETS_B/MIR_DATASETS_B\")\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        npy_path = Path(result[\"image_id\"])\n",
    "        classe = result[\"classe\"]\n",
    "        score = result[\"score\"]\n",
    "\n",
    "        parts = npy_path.name.split(\"_\")\n",
    "        subclasse = parts[3]\n",
    "        image_name = \"_\".join(parts[:-1]) + \".jpg\"\n",
    "\n",
    "        # Construct full image path\n",
    "        original_img_path = original_root / classe / subclasse / image_name\n",
    "\n",
    "        if not original_img_path.exists():\n",
    "            print(f\"[!] Missing image file: {original_img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(original_img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.subplot(1, n + 1, i + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{classe}\\nscore: {score:.1f}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"MIR_DATASETS_B/MIR_DATASETS_B/chiens/Chihuahua/1_3_chiens_Chihuahua_1271.jpg\"\n",
    "results = query_image(query, top_k=5, ensemble_config={\"hist\": 0.5, \"orb\": 0.5})\n",
    "show_query_results(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e6167",
   "metadata": {},
   "source": [
    "## > Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4c59b",
   "metadata": {},
   "source": [
    "#### Generate test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb240698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Base path to original images\n",
    "base_img_dir = Path(\"MIR_DATASETS_B/MIR_DATASETS_B\")\n",
    "\n",
    "# Pick 1 image per class (manual or automated)\n",
    "examples = [\n",
    "    (\"araignees\", \"barn spider\", \"0_0_araignees_barnspider_1.jpg\"),\n",
    "    (\"chiens\", \"Chihuahua\", \"1_3_chiens_Chihuahua_1271.jpg\"),\n",
    "    (\"oiseaux\", \"bulbul\", \"2_5_oiseaux_bulbul_2512.jpg\"),\n",
    "    (\"poissons\", \"hammerhead\", \"3_5_poissons_hammerhead_3414.jpg\"),\n",
    "    (\"singes\", \"gorilla\", \"4_2_singes_gorilla_3890.jpg\")\n",
    "]\n",
    "\n",
    "# Build rows for the CSV\n",
    "rows = []\n",
    "for classe, subclasse, filename in examples:\n",
    "    path = base_img_dir / classe / subclasse / filename\n",
    "    rows.append({\n",
    "        \"query_image_path\": str(path),\n",
    "        \"expected_class\": classe\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"test_queries.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved test_queries.csv with 5 query samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cedcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"test_queries.csv\")\n",
    "\n",
    "for path_str in df[\"query_image_path\"]:\n",
    "    path = Path(path_str.strip())\n",
    "    print(f\"{path}: Exists={path.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea3cb0",
   "metadata": {},
   "source": [
    "#### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_average_precision(results, expected_class):\n",
    "    \"\"\"\n",
    "    Compute average precision (AP) for a single query.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    precisions = []\n",
    "    for i, r in enumerate(results):\n",
    "        if r[\"classe\"] == expected_class:\n",
    "            hits += 1\n",
    "            precisions.append(hits / (i + 1))\n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "def compute_r_precision(results, expected_class, R):\n",
    "    \"\"\"\n",
    "    R-Precision = Precision at R, where R = number of relevant items.\n",
    "    Since we only expect class match, assume R = total items with same class in the dataset.\n",
    "    For simplification in top_k, we approximate R = hits in top_k.\n",
    "    \"\"\"\n",
    "    relevant = [r for r in results if r[\"classe\"] == expected_class]\n",
    "    R_estimated = len(relevant)\n",
    "    return sum([1 for r in results[:R_estimated] if r[\"classe\"] == expected_class]) / max(R_estimated, 1)\n",
    "\n",
    "def evaluate(test_csv_path, top_k=10, ensemble_config=None, save_csv=True):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval system based on a test CSV.\n",
    "    \n",
    "    Returns a DataFrame with individual and global metrics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    rows = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        query_path = str(Path(row[\"query_image_path\"]))\n",
    "        expected_class = row[\"expected_class\"]\n",
    "\n",
    "        results = query_image(query_path, top_k=top_k, ensemble_config=ensemble_config)\n",
    "\n",
    "        topk_hits = sum([1 for r in results[:top_k] if r[\"classe\"] == expected_class])\n",
    "        recall_at_k = topk_hits / top_k\n",
    "        precision_at_k = topk_hits / top_k\n",
    "        ap = compute_average_precision(results[:top_k], expected_class)\n",
    "        rp = compute_r_precision(results[:top_k], expected_class, R=None)\n",
    "\n",
    "        rows.append({\n",
    "            \"query_image\": query_path,\n",
    "            \"expected_class\": expected_class,\n",
    "            \"precision@k\": precision_at_k,\n",
    "            \"recall@k\": recall_at_k,\n",
    "            \"AP\": ap,\n",
    "            \"R-Precision\": rp\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Add global metrics\n",
    "    avg_metrics = results_df[[\"precision@k\", \"recall@k\", \"AP\", \"R-Precision\"]].mean()\n",
    "    avg_metrics[\"query_image\"] = \"MEAN\"\n",
    "    avg_metrics[\"expected_class\"] = \"-\"\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([avg_metrics])], ignore_index=True)\n",
    "\n",
    "    if save_csv:\n",
    "        results_df.to_csv(\"retrieval_evaluation_results.csv\", index=False)\n",
    "        print(\"âœ… Saved: retrieval_evaluation_results.csv\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = evaluate(\"test_queries.csv\", top_k=5, ensemble_config={\"hist\": 0.5, \"orb\": 0.5})\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6227f9",
   "metadata": {},
   "source": [
    "## > Try to maximize the result by the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def generate_weight_combinations(step=0.1):\n",
    "    values = [round(i * step, 2) for i in range(int(1 / step) + 1)]\n",
    "    combos = []\n",
    "    for h, o, s in itertools.product(values, repeat=3):\n",
    "        if abs(h + o + s - 1.0) < 1e-6:\n",
    "            combos.append({\"hist\": h, \"orb\": o, \"sift\": s})\n",
    "    return combos\n",
    "\n",
    "def run_grid_search(test_csv=\"test_queries.csv\", top_k=5):\n",
    "    weight_combos = generate_weight_combinations(step=0.1)\n",
    "    results = []\n",
    "\n",
    "    for weights in weight_combos:\n",
    "        print(f\">>> Testing ensemble: {weights}\")\n",
    "        start = time.time()\n",
    "        df_eval = evaluate(test_csv, top_k=top_k, ensemble_config=weights, save_csv=False)\n",
    "        end = time.time()\n",
    "\n",
    "        # Linha com mÃ©tricas globais (Ãºltima linha do df_eval)\n",
    "        avg_row = df_eval[df_eval[\"query_image\"] == \"MEAN\"].iloc[0]\n",
    "        results.append({\n",
    "            \"hist_weight\": weights[\"hist\"],\n",
    "            \"orb_weight\": weights[\"orb\"],\n",
    "            \"sift_weight\": weights[\"sift\"],\n",
    "            \"mAP\": avg_row[\"AP\"],\n",
    "            \"precision@k\": avg_row[\"precision@k\"],\n",
    "            \"recall@k\": avg_row[\"recall@k\"],\n",
    "            \"R-Precision\": avg_row[\"R-Precision\"],\n",
    "            \"time_sec\": round(end - start, 2)\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"ensemble_results.csv\", index=False)\n",
    "    print(\"âœ… Saved: ensemble_results.csv\")\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4107bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_grid_search(\"test_queries.csv\", top_k=5)\n",
    "print(df.sort_values(\"mAP\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f098584",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9e6f",
   "metadata": {},
   "source": [
    "## > Vision Transformer (ViT) - Vector extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6169b5",
   "metadata": {},
   "source": [
    "Let's extract vectors from deep visual characteristics using a Vision Transformer (VIT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd469b8a",
   "metadata": {},
   "source": [
    "We cannot reuse the existing .npy files, as VIT produces distinct embeddings that should be generated from scratch. Based on the last output of the past, Swift was the descriptor that did better, so let's compare with him in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80363c6",
   "metadata": {},
   "source": [
    "### Structuring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84ac81",
   "metadata": {},
   "source": [
    "#### Creating the descriptors (feature_output_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ab7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "# Load the ViT model and feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.eval()\n",
    "\n",
    "# Define input and output folders\n",
    "input_root = \"features_output\"\n",
    "image_root = \"MIR_DATASETS_B/MIR_DATASETS_B\"\n",
    "output_root = \"feature_output_vit\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Loop over class folders\n",
    "for class_name in os.listdir(input_root):\n",
    "    class_input_path = os.path.join(input_root, class_name)\n",
    "    class_output_path = os.path.join(output_root, class_name)\n",
    "    os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing class: {class_name}\")\n",
    "\n",
    "    npy_files = [f for f in os.listdir(class_input_path) if f.endswith(\"_sift.npy\")]\n",
    "\n",
    "    for npy_file in tqdm(npy_files):\n",
    "        base_name = npy_file.replace(\"_sift.npy\", \"\")\n",
    "        # Use glob to find the image recursively within the class folder\n",
    "        search_pattern = os.path.join(image_root, class_name, \"**\", base_name + \".jpg\")\n",
    "        matching_files = glob(search_pattern, recursive=True)\n",
    "\n",
    "        if not matching_files:\n",
    "            print(f\"[!] Image not found for {base_name}\")\n",
    "            continue\n",
    "\n",
    "        image_path = matching_files[0]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = vit_model(**inputs)\n",
    "            embedding = outputs.pooler_output.squeeze().numpy()\n",
    "\n",
    "        output_file = os.path.join(class_output_path, base_name + \".npy\")\n",
    "        np.save(output_file, embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfb03d",
   "metadata": {},
   "source": [
    "#### Creating the vectors by class (.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Root directory where ViT features were saved\n",
    "features_root = \"feature_output_vit\"\n",
    "\n",
    "# Loop over each class directory\n",
    "for class_name in os.listdir(features_root):\n",
    "    class_path = os.path.join(features_root, class_name)\n",
    "    \n",
    "    # Skip if not a directory\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Creating parquet for class: {class_name}\")\n",
    "    \n",
    "    embeddings = []\n",
    "    image_ids = []\n",
    "\n",
    "    # Read all .npy files in the class folder\n",
    "    for fname in tqdm(os.listdir(class_path)):\n",
    "        if not fname.endswith(\".npy\"):\n",
    "            continue\n",
    "\n",
    "        fpath = os.path.join(class_path, fname)\n",
    "        vector = np.load(fpath)\n",
    "\n",
    "        image_id = fname.replace(\".npy\", \"\")\n",
    "        embeddings.append(vector)\n",
    "        image_ids.append(image_id)\n",
    "\n",
    "    # Create a DataFrame with image_id as index\n",
    "    df = pd.DataFrame(embeddings, index=image_ids)\n",
    "    df.index.name = \"image_id\"\n",
    "\n",
    "    # Save as .parquet\n",
    "    parquet_path = os.path.join(features_root, f\"{class_name}.parquet\")\n",
    "    df.to_parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daebe76",
   "metadata": {},
   "source": [
    "#### Creating vector of text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b392235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "# Load the captions JSON\n",
    "with open(\"captions_MIR_DATASETS_B.json\", \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Store results\n",
    "text_embeddings = []\n",
    "image_ids = []\n",
    "\n",
    "# Extract embeddings\n",
    "for image_path, caption in tqdm(captions.items()):\n",
    "    # Format image ID (remove extension and keep only filename)\n",
    "    image_id = os.path.basename(image_path).replace(\".jpg\", \"\")\n",
    "\n",
    "    # Tokenize and extract text embedding\n",
    "    inputs = clip_processor(text=[caption], return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_text_features(**inputs)\n",
    "        embedding = outputs.squeeze().numpy()  # shape: (512,)\n",
    "\n",
    "    image_ids.append(image_id)\n",
    "    text_embeddings.append(embedding)\n",
    "\n",
    "# Save all text embeddings into a single parquet\n",
    "df = pd.DataFrame(text_embeddings, index=image_ids)\n",
    "df.index.name = \"image_id\"\n",
    "df.to_parquet(\"clip_text_embeddings.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459bdc3",
   "metadata": {},
   "source": [
    "#### Vector indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d968cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho das pastas\n",
    "features_root = \"feature_output_vit\"\n",
    "index_output_path = \"index_store\"\n",
    "\n",
    "# Crie a pasta se nÃ£o existir\n",
    "os.makedirs(index_output_path, exist_ok=True)\n",
    "\n",
    "all_vectors = []\n",
    "all_ids = []\n",
    "\n",
    "# LÃª todos os .parquet por classe\n",
    "for fname in os.listdir(features_root):\n",
    "    if not fname.endswith(\".parquet\"):\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_parquet(os.path.join(features_root, fname))\n",
    "    all_vectors.append(df.values)\n",
    "    all_ids.extend(df.index.tolist())\n",
    "\n",
    "# Empilha tudo em um Ãºnico array\n",
    "vectors = np.vstack(all_vectors).astype(\"float32\")  # FAISS exige float32\n",
    "\n",
    "# Cria o Ã­ndice FAISS\n",
    "dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors)\n",
    "\n",
    "# Salva o Ã­ndice\n",
    "faiss.write_index(index, os.path.join(index_output_path, \"index_vit.faiss\"))\n",
    "\n",
    "# Salva os nomes das imagens\n",
    "df_ids = pd.DataFrame({\"image_id\": all_ids})\n",
    "df_ids.to_parquet(os.path.join(index_output_path, \"ids_vit.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373a86e",
   "metadata": {},
   "source": [
    "#### Creating faiss index with clip textual vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Caminhos\n",
    "embedding_file = \"clip_text_embeddings.parquet\"\n",
    "index_output_path = \"index_store\"\n",
    "os.makedirs(index_output_path, exist_ok=True)\n",
    "\n",
    "# Carrega os embeddings textuais\n",
    "df = pd.read_parquet(embedding_file)\n",
    "vectors = df.values.astype(\"float32\")\n",
    "ids = df.index.tolist()\n",
    "\n",
    "# Cria Ã­ndice FAISS (512 dimensÃµes)\n",
    "dimension = vectors.shape[1]\n",
    "assert dimension == 512, f\"Expected dimension 512, got {dimension}\"\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors)\n",
    "\n",
    "# Salva o Ã­ndice\n",
    "faiss.write_index(index, os.path.join(index_output_path, \"index_clip_text.faiss\"))\n",
    "\n",
    "# Salva os image_ids\n",
    "df_ids = pd.DataFrame({\"image_id\": ids})\n",
    "df_ids.to_parquet(os.path.join(index_output_path, \"ids_clip_text.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96844c32",
   "metadata": {},
   "source": [
    "## Searching by text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "# Load the FAISS index and image IDs (for text embeddings)\n",
    "index = faiss.read_index(\"index_store/index_clip_text.faiss\")\n",
    "df_ids = pd.read_parquet(\"index_store/ids_clip_text.parquet\")\n",
    "\n",
    "# --- Function to search by text in the textual embedding space ---\n",
    "def search_by_text(text_query, top_k=5):\n",
    "    \"\"\"\n",
    "    Given a text query, return the top-k most semantically similar image captions.\n",
    "\n",
    "    Args:\n",
    "        text_query (str): The natural language search string.\n",
    "        top_k (int): Number of closest results to return.\n",
    "\n",
    "    Returns:\n",
    "        results (list): List of image_ids ranked by similarity.\n",
    "        distances (list): Corresponding distances in vector space.\n",
    "    \"\"\"\n",
    "    # Convert the query into a CLIP embedding\n",
    "    inputs = clip_processor(text=[text_query], return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = clip_model.get_text_features(**inputs).numpy().astype(\"float32\")\n",
    "\n",
    "    # Search for nearest neighbors in FAISS index\n",
    "    distances, indices = index.search(text_embedding, top_k)\n",
    "\n",
    "    # Retrieve image IDs from index positions\n",
    "    results = [df_ids.iloc[idx][\"image_id\"] for idx in indices[0]]\n",
    "    return results, distances[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fa62a",
   "metadata": {},
   "source": [
    "#### Use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, distances = search_by_text(\"a monkey eating a banana\", top_k=10)\n",
    "print(\"Top results:\")\n",
    "for img, dist in zip(results, distances):\n",
    "    print(f\"{img}  (distance={dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8058141",
   "metadata": {},
   "source": [
    "##### Showing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Base directory where the original images are stored\n",
    "image_base_path = \"MIR_DATASETS_B/MIR_DATASETS_B\"\n",
    "\n",
    "# --- Function to locate full path of a given image_id ---\n",
    "def find_image_path(image_id):\n",
    "    \"\"\"\n",
    "    Search for the image path in the base directory using the image ID.\n",
    "    It recursively searches through subfolders.\n",
    "\n",
    "    Args:\n",
    "        image_id (str): The image filename without extension.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Full path to the image if found, else None.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(image_base_path):\n",
    "        for fname in files:\n",
    "            if fname.startswith(image_id) and fname.endswith(\".jpg\"):\n",
    "                return os.path.join(root, fname)\n",
    "    return None\n",
    "\n",
    "# --- Function to show search results as images ---\n",
    "def show_results(image_ids, title=\"Top Results\"):\n",
    "    \"\"\"\n",
    "    Display images for the given image IDs.\n",
    "\n",
    "    Args:\n",
    "        image_ids (list): List of image IDs to show.\n",
    "        title (str): Title to display above the image set.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, img_id in enumerate(image_ids):\n",
    "        img_path = find_image_path(img_id)\n",
    "        if img_path is None:\n",
    "            print(f\"[!] Not found: {img_id}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        plt.subplot(1, len(image_ids), i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(img_id, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, distances = search_by_text(\"a monkey eating a banana\", top_k=5)\n",
    "show_results(results, title=\"Top 5: monkey eating a banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3fe59",
   "metadata": {},
   "source": [
    "## Evaluating the text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ddc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_from_image_id(image_id):\n",
    "    \"\"\"\n",
    "    Extracts the class label from an image ID.\n",
    "    Assumes the ID format is: 'prefix_class_subclass_index'\n",
    "\n",
    "    Example:\n",
    "        '0_2_singes_monkey_123' â†’ 'singes'\n",
    "\n",
    "    Returns:\n",
    "        str: top-level class name\n",
    "    \"\"\"\n",
    "    parts = image_id.split('_')\n",
    "    if len(parts) < 3:\n",
    "        return \"unknown\"\n",
    "    return parts[2]  # Ex: 'singes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_relevant_images(class_name):\n",
    "    \"\"\"\n",
    "    Count how many total images exist in the dataset for a given class.\n",
    "    Based on IDs in the FAISS index.\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return sum(1 for img_id in df_ids[\"image_id\"] if extract_class_from_image_id(img_id) == class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision(relevance_list):\n",
    "    \"\"\"\n",
    "    Compute average precision for a binary relevance list.\n",
    "\n",
    "    Args:\n",
    "        relevance_list (list of 0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        float: AP\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    rel_count = 0\n",
    "    for i, rel in enumerate(relevance_list):\n",
    "        if rel:\n",
    "            rel_count += 1\n",
    "            ap += rel_count / (i + 1)\n",
    "    return ap / rel_count if rel_count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_query(query_text, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance for a single text query.\n",
    "\n",
    "    Steps:\n",
    "    - Use the text to query the FAISS index\n",
    "    - Extract the true class from the text's image_id (via caption mapping)\n",
    "    - Compare retrieved image_ids against ground truth class\n",
    "\n",
    "    Returns:\n",
    "        dict with precision@k, recall@k, AP, R-Precision\n",
    "    \"\"\"\n",
    "    # Get the ground truth image from the caption list\n",
    "    # We assume you still have `captions` loaded\n",
    "    matched = [(img_id, cap) for img_id, cap in captions.items() if cap == query_text]\n",
    "    if not matched:\n",
    "        print(\"[!] Caption not found in JSON.\")\n",
    "        return None\n",
    "\n",
    "    query_image_id = os.path.basename(matched[0][0]).replace(\".jpg\", \"\")\n",
    "    true_class = extract_class_from_image_id(query_image_id)\n",
    "\n",
    "    # Run the search\n",
    "    retrieved_ids, _ = search_by_text(query_text, top_k=top_k)\n",
    "\n",
    "    # Evaluate\n",
    "    relevant = [1 if extract_class_from_image_id(rid) == true_class else 0 for rid in retrieved_ids]\n",
    "    precision_at_k = sum(relevant) / len(relevant)\n",
    "    recall = sum(relevant) / get_total_relevant_images(true_class)\n",
    "    r_precision = sum(relevant[:get_total_relevant_images(true_class)]) / get_total_relevant_images(true_class)\n",
    "    ap = compute_average_precision(relevant)\n",
    "\n",
    "    return {\n",
    "        \"query\": query_text,\n",
    "        \"true_class\": true_class,\n",
    "        \"precision@k\": precision_at_k,\n",
    "        \"recall@k\": recall,\n",
    "        \"AP\": ap,\n",
    "        \"R-Precision\": r_precision,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe308cb",
   "metadata": {},
   "source": [
    "#### Just one case evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0eb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_single_query(\"a monkey eating a banana\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e8dba",
   "metadata": {},
   "source": [
    "#### Evaluating some cases at same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e175cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_queries(captions_dict, top_k=10, max_queries=100):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality over multiple text queries.\n",
    "\n",
    "    Args:\n",
    "        captions_dict (dict): Mapping of image_path â†’ caption\n",
    "        top_k (int): Number of top results to consider for each query\n",
    "        max_queries (int): Maximum number of queries to evaluate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with metrics per query and overall averages\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    all_results = []\n",
    "    used_captions = set()\n",
    "    query_count = 0\n",
    "\n",
    "    for image_path, caption in tqdm(captions_dict.items()):\n",
    "        # Avoid duplicates\n",
    "        if caption in used_captions:\n",
    "            continue\n",
    "        used_captions.add(caption)\n",
    "\n",
    "        result = evaluate_single_query(caption, top_k=top_k)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            query_count += 1\n",
    "\n",
    "        if query_count >= max_queries:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Compute aggregated metrics\n",
    "    summary = {\n",
    "        \"mean precision@k\": df[\"precision@k\"].mean(),\n",
    "        \"mean recall@k\": df[\"recall@k\"].mean(),\n",
    "        \"mean AP (mAP)\": df[\"AP\"].mean(),\n",
    "        \"mean R-Precision\": df[\"R-Precision\"].mean(),\n",
    "        \"evaluated_queries\": len(df),\n",
    "    }\n",
    "\n",
    "    return df, pd.Series(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar atÃ© 100 legendas diferentes, com top_k = 10\n",
    "df_metrics, summary = evaluate_multiple_queries(captions, top_k=10, max_queries=100)\n",
    "\n",
    "# Ver resumo\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7dac7",
   "metadata": {},
   "source": [
    "### Creating the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c814796",
   "metadata": {},
   "source": [
    "#### Descriptors comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lista completa de descritores\n",
    "descriptors = {\n",
    "    \"SIFT\": {\n",
    "        \"index_file\": \"index_store/index_sift.faiss\",\n",
    "        \"ids_file\": \"index_store/ids_sift.parquet\",\n",
    "    },\n",
    "    \"ORB\": {\n",
    "        \"index_file\": \"index_store/index_orb.faiss\",\n",
    "        \"ids_file\": \"index_store/ids_orb.parquet\",\n",
    "    },\n",
    "    \"HIST\": {\n",
    "        \"index_file\": \"index_store/index_hist.faiss\",\n",
    "        \"ids_file\": \"index_store/ids_hist.parquet\",\n",
    "    },\n",
    "    \"ViT\": {\n",
    "        \"index_file\": \"index_store/index_vit.faiss\",\n",
    "        \"ids_file\": \"index_store/ids_vit.parquet\",\n",
    "    },\n",
    "    \"CLIP-text\": {\n",
    "        \"index_file\": \"index_store/index_clip_text.faiss\",\n",
    "        \"ids_file\": \"index_store/ids_clip_text.parquet\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def average_search_time(index, num_queries=100):\n",
    "    dim = index.d\n",
    "    queries = np.random.rand(num_queries, dim).astype(\"float32\")\n",
    "    start = time.time()\n",
    "    index.search(queries, k=5)\n",
    "    end = time.time()\n",
    "    return (end - start) / num_queries\n",
    "\n",
    "# ConstruÃ§Ã£o da tabela\n",
    "rows = []\n",
    "\n",
    "for i, (desc_name, paths) in enumerate(descriptors.items(), start=1):\n",
    "    index_path = paths[\"index_file\"]\n",
    "    ids_path = paths[\"ids_file\"]\n",
    "\n",
    "    # Tempo de carregamento do Ã­ndice\n",
    "    start_index_time = time.time()\n",
    "    index = faiss.read_index(index_path)\n",
    "    index_load_time = time.time() - start_index_time\n",
    "\n",
    "    # Tempo mÃ©dio de busca\n",
    "    avg_search = average_search_time(index)\n",
    "\n",
    "    # Tamanho dos arquivos\n",
    "    total_size_mb = (\n",
    "        os.path.getsize(index_path) + os.path.getsize(ids_path)\n",
    "    ) / 1024 / 1024\n",
    "\n",
    "    rows.append({\n",
    "        \"Vos meilleurs descripteurs\": f\"Descripteur NÂ° {i:02}\",\n",
    "        \"Nom de(s) descripteur(s)\": desc_name,\n",
    "        \"Temps dâ€™indexation (s)\": round(index_load_time, 3),\n",
    "        \"Taille du descripteur (MB)\": round(total_size_mb, 2),\n",
    "        \"Temps de recherche moyen par image (s)\": round(avg_search, 5)\n",
    "    })\n",
    "\n",
    "# DataFrame final\n",
    "df_table1 = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656724e2",
   "metadata": {},
   "source": [
    "#### Image searching table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our group of images\n",
    "query_requests = {\n",
    "    \"R1\": \"3_4_poissons_eagleray_3310\",\n",
    "    \"R2\": \"3_5_poissons_hammerhead_3495\",\n",
    "    \"R3\": \"3_3_poissons_tigershark_3244\",\n",
    "    \"R4\": \"1_2_chiens_boxer_1146\",\n",
    "    \"R5\": \"1_4_chiens_goldenretriever_1423\",\n",
    "    \"R6\": \"1_5_chiens_Rottweiler_1578\",\n",
    "    \"R7\": \"4_3_singes_squirrelmonkey_4082\",\n",
    "    \"R8\": \"4_2_singes_gorilla_4004\",\n",
    "    \"R9\": \"4_1_singes_chimpanzee_3772\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f412537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FunÃ§Ãµes utilitÃ¡rias\n",
    "def extract_class_from_image_id(image_id):\n",
    "    return image_id.split('_')[2]  # ex: 'poissons'\n",
    "\n",
    "def get_total_relevant_images(class_name, df_ids):\n",
    "    return sum(1 for img_id in df_ids[\"image_id\"] if extract_class_from_image_id(img_id) == class_name)\n",
    "\n",
    "def compute_average_precision(relevance_list, total_relevant):\n",
    "    \"\"\"\n",
    "    Computes Average Precision (AP) over the top-k results,\n",
    "    normalized by the number of total relevant items in the dataset.\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    hit_count = 0\n",
    "\n",
    "    for i, rel in enumerate(relevance_list):\n",
    "        if rel:\n",
    "            hit_count += 1\n",
    "            precision_at_i = hit_count / (i + 1)\n",
    "            ap += precision_at_i\n",
    "\n",
    "    return ap / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_vit_query(image_id, index, df_ids, feature_dir, top_ks=[50, 100]):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval metrics for a given query image using ViT features.\n",
    "\n",
    "    Args:\n",
    "        image_id (str): The ID of the query image (without .jpg)\n",
    "        index (faiss.Index): FAISS index loaded for the descriptor\n",
    "        df_ids (DataFrame): Mapping from FAISS positions to image_ids\n",
    "        feature_dir (str): Directory where .npy features are stored\n",
    "        top_ks (list): List of K values to evaluate (e.g., [50, 100])\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with precision, recall, AP and TopMax for each K\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    class_name = extract_class_from_image_id(image_id)\n",
    "\n",
    "    # Load the ViT feature vector of the query image\n",
    "    query_vector = None\n",
    "    for root, _, files in os.walk(feature_dir):\n",
    "        for f in files:\n",
    "            if f.startswith(image_id) and f.endswith(\".npy\"):\n",
    "                query_vector = np.load(os.path.join(root, f)).astype(\"float32\").reshape(1, -1)\n",
    "                break\n",
    "        if query_vector is not None:\n",
    "            break\n",
    "\n",
    "    if query_vector is None:\n",
    "        print(f\"[!] Feature not found for {image_id}\")\n",
    "        return None\n",
    "\n",
    "    result = {\"Indice requÃªte\": image_id}\n",
    "    total_relevant = get_total_relevant_images(class_name, df_ids)\n",
    "    result[\"TopMax\"] = total_relevant\n",
    "\n",
    "    # Retrieve more than max(top_ks) to allow for exclusion\n",
    "    max_k = max(top_ks)\n",
    "    distances, indices = index.search(query_vector, max_k + 10)\n",
    "\n",
    "    # Get image IDs of retrieved items\n",
    "    retrieved_ids = [df_ids.iloc[i][\"image_id\"] for i in indices[0]]\n",
    "\n",
    "    # Remove all images with the same exact subclass (prefix match)\n",
    "    prefix = \"_\".join(image_id.split(\"_\")[:4])  # Ex: '3_4_poissons_eagleray'\n",
    "    retrieved_ids = [rid for rid in retrieved_ids if not rid.startswith(prefix)]\n",
    "\n",
    "\n",
    "    # Recalculate metrics using cleaned list\n",
    "    for k in top_ks:\n",
    "        rel_k_ids = retrieved_ids[:k]\n",
    "        relevance_k = [1 if extract_class_from_image_id(rid) == class_name else 0 for rid in rel_k_ids]\n",
    "\n",
    "        precision = sum(relevance_k) / k\n",
    "        recall = sum(relevance_k) / total_relevant\n",
    "        ap = compute_average_precision(relevance_k, total_relevant)\n",
    "\n",
    "        result[f\"P (Top{k})\"] = round(precision, 3)\n",
    "        result[f\"R (Top{k})\"] = round(recall, 3)\n",
    "        result[f\"AP (Top{k})\"] = round(ap, 3)\n",
    "\n",
    "    result[\"MaP (Top50)\"] = result[\"AP (Top50)\"]\n",
    "    result[\"MaP (Top100)\"] = result[\"AP (Top100)\"]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ab797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o Ã­ndice ViT\n",
    "index = faiss.read_index(\"index_store/index_vit.faiss\")\n",
    "df_ids = pd.read_parquet(\"index_store/ids_vit.parquet\")\n",
    "feature_dir = \"feature_output_vit\"\n",
    "\n",
    "# AvaliaÃ§Ã£o das 9 requisiÃ§Ãµes\n",
    "tabela2_rows = []\n",
    "for req_name, image_id in tqdm(query_requests.items()):\n",
    "    row = evaluate_vit_query(image_id, index, df_ids, feature_dir)\n",
    "    row[\"Indice requÃªte\"] = req_name\n",
    "    tabela2_rows.append(row)\n",
    "\n",
    "# Criar DataFrame com ordem de colunas\n",
    "df_table2 = pd.DataFrame(tabela2_rows)[[\n",
    "    \"Indice requÃªte\",\n",
    "    \"R (Top50)\", \"R (Top100)\",\n",
    "    \"P (Top50)\", \"P (Top100)\",\n",
    "    \"AP (Top50)\", \"AP (Top100)\",\n",
    "    \"MaP (Top50)\", \"MaP (Top100)\",\n",
    "    \"TopMax\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_table2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
